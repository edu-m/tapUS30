{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581dc5c-450a-4752-afca-e4f195d4972e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Progetto di Technologies for Advanced Programming \"_TAP US30_\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2ea00",
   "metadata": {},
   "source": [
    "**BATCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b3286",
   "metadata": {},
   "source": [
    "La fase di batch è considerata la fase iniziale del progetto. Essa pone la base con la quale sarà possibile visualizzare i dati storici di ciascun settore, insieme ad una finestra temporale entro la quale saranno collocate le previsioni calcolate in questa fase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f0a3b-a4ca-484c-92f6-3366565f7555",
   "metadata": {},
   "source": [
    "CLIENT PHP\n",
    "\n",
    "Il primo passo della pipeline è quello di creare un client che s'interfacci\n",
    "con l'API di Polygon e faccia richiesta dei dati, considerando la sola giornata \n",
    "precedente. Quest'ultimo sarà mandato in esecuzione su un container.Di seguito, \n",
    "vediamo come ottenere l'opportuno Dockerfile, da cui potremo creare l'immagine \n",
    "che, nel nostro \"docker-compose\", chiameremo \"producer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942c313-2c6b-406a-8b42-2acd2d36671c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FROM alpine\n",
    "RUN apk update && apk add php composer php-fileinfo\n",
    "WORKDIR /app\n",
    "RUN composer require polygon-io/api -W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969dc170-6bb6-4264-9177-7f414cb8c01a",
   "metadata": {},
   "source": [
    "L'immagine base è quella della versione \"alpine\" di Linux, scelta in quanto\n",
    "tra le più \"leggere\" in termini di spazio sul disco. Il gestore di pacchetti\n",
    "si chiama \"apk\", da cui installiamo le librerie php e composer. Eseguiamo il comando \"composer\" per richiedere\n",
    "la libreria da utilizzare nel nostro codice php così da accedere all'API.\n",
    "Dopodichè, una volta partito il container, verrà eseguto da docker-compose il comando che\n",
    "innesca l'esecuzione del suddetto codice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486a558",
   "metadata": {},
   "source": [
    "*batch_extract.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "$tickers = array(\n",
    "    \"AXP\" => \"financial\",\n",
    "    \"AMGN\" => \"health\",\n",
    "    \"AAPL\" => \"tech\",\n",
    "    \"BA\" => \"industrial\",\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    ");\n",
    "\n",
    "$currentDate = new DateTime();\n",
    "$startingDate = new DateTime();\n",
    "$interval = new DateInterval('P5Y');\n",
    "$startingDate->sub($interval);\n",
    "\n",
    "$rest = new Rest($api_key);\n",
    "\n",
    "function write_batch($data_array, $ticker, $category, $tickers)\n",
    "{\n",
    "    $path = \"/data/raw/$category\";\n",
    "    $filename = \"$ticker.txt\";\n",
    "    if(!file_exists($path))\n",
    "        mkdir($path);\n",
    "    $file = fopen($path.\"/\".$filename, \"w\") or die(\"Unable to open file !\");\n",
    "    $results = $data_array[\"results\"];\n",
    "    foreach ($results as $result) {\n",
    "        $result[\"tickerSymbol\"] = $ticker;\n",
    "        fwrite($file, json_encode($result) . \"\\n\");\n",
    "    }\n",
    "    fclose($file);\n",
    "}\n",
    "\n",
    "foreach ($tickers as $ticker => $category) {\n",
    "    sleep(1);\n",
    "    $data = $rest->stocks->aggregates->get(\n",
    "        $ticker,\n",
    "        1,\n",
    "        $startingDate->format('Y-m-d'),\n",
    "        $currentDate->format('Y-m-d'),\n",
    "        'day'\n",
    "    );\n",
    "    write_batch($data, $ticker, $category, $tickers);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79a18e-868e-4eb6-b8ed-acaa7a8e1017",
   "metadata": {},
   "source": [
    "Creiamo un array associativo con come chiave il nome dell'azienda e come valore il settore in cui\n",
    "opera. Dichiariamo poi un oggetto di tipo \"Rest\" sul quale chiameremo una serie di\n",
    "funzioni per prelevare i dati che ci occorrono. Il più importante è il metodo \"get\"\n",
    "che richiede come parametri il nome dell'azienda, un intero, la data di inizio e \n",
    "quella di fine entro cui prelevare le informazioni e l'unità di tempo, che indichiamo\n",
    "come il singolo giorno. In un ciclo for avanzato, scorriamo l'array associativo. ogni\n",
    "12 secondi, inviamo una richiesta attraverso l'API. Salviamo queste informazioni su una\n",
    "variabile che viene passata alla nostra funzione \"write_batch\", insieme al nome \n",
    "dell'azienda e alla categoria a cui appartiene, aggiungendo poi anche l'array\n",
    "associativo stesso.\n",
    "\n",
    "Dopo aver proceduto, alla creazione della cartella e del file, si salva in una variabile\n",
    "l'array alla posizione \"results\" dell'array ottenuto dalla get. Si effettua un ciclo su\n",
    "tutte le n-uple che caratterizzano i dati della specifica azienda, a partire dall'array\n",
    "salvato nella variabile e, dopo aver aggiunto il ticker symbol in questione (dato che \n",
    "altrimenti non sarebbe presente per ogni n-upla, il che creerebbe dei problemi per il \n",
    "successivo processing), si scrive sul file appena creato il contenuto dell'array, \n",
    "sottoforma di JSON. Fuori dal ciclo, si procede alla chiusura del file.\n",
    "Quello che otterremo è quindi la presenza di 6 cartelle, una per ogni settore a cui le\n",
    "30 aziende appartengono, e ognuna di queste cartelle avrà dei file con estensione \"txt\" \n",
    "composti da n JSON, uno per ogni giorno in cui ogni azienda opera nel mercato azionario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a64cc-e122-4f80-bc04-c4b954dca877",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549143b8-dc9b-48f7-9cc8-9ff559fb7f2d",
   "metadata": {},
   "source": [
    "*process.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a04a1-fb21-409d-966c-58e3b68c2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"tapus30\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "names=[\"cgoods\",\"financial\",\"energy\",\"health\",\"industrial\",\"tech\"]\n",
    "indexes=[Elasticsearch(\"http://es_cgoods:9200\"),Elasticsearch(\"http://es_financial:9200\"),Elasticsearch(\"http://es_energy:9200\"),Elasticsearch(\"http://es_health:9200\"),Elasticsearch(\"http://es_industrial:9200\"),Elasticsearch(\"http://es_tech:9200\")]\n",
    "prediction_data=[]\n",
    "historical_data=[]\n",
    "day_in_ms = 86400000\n",
    "window_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405700e-73d6-4b46-a3c9-9a56488ff4b0",
   "metadata": {},
   "source": [
    "Innanzitutto, apriamo una sessione di Spark, scegliendo come master il nostro stesso container. Specifichiamo il nome del nostro driver e salviamo le impostazioni sull'oggetto \"spark\". Su questo verra richiamato \"sparkContext\" per l'apertura effettiva del context e salviamo ciò che ci viene restituito sulla variabile \"sc\", su cui chiameremo poi il metodo \"setLogLevel\", per decidere il tipo di logs che vogliamo siano prodotti all'esecuzione del codice.\n",
    "Creiamo poi un array con i nomi dei settori, che saranno anche i nomi degli indici su elasticsearch. Dichiariamo un array di oggetti Elasticsearch attraverso cui, successivamente, potremo effettivamente inviare i dati alle sue sei diverse istanze, ancora una volta una per settore. \n",
    "Dichiariamo poi due array vuoti, che saranno popolati dai dati restituitici dal modello di machine learning che useremo (prediction_data) e dai dati che preleveremo dal nostro file system, relativi ai 2 anni di operazioni in borsa (historical_data). \n",
    "Impostiamo un offset fondamentale per operare con i timestamp (ovvero le date espresse in millisecondi, a partire dall'omonimo campo all'interno dei JSON), cioè quello che ci consente di aggiungere un giorno alla data da cui partiamo. Altra importante variabile è quella \"window_size\", che ci permetterà di eseguire la funzione ricorsiva che vedremo per implementare una sliding window di 12 giorni, base su cui il modello via via restituirà l'ultima previsione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dd518-fc8b-4424-90c3-1afbf480f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    df = spark.read.JSON(\"/data/\"+name)\n",
    "    assembler = VectorAssembler(inputCols=['open','high','low'],outputCol='features')\n",
    "    output = assembler.transform(df).select('features','close','tickerSymbol','timestamp')\n",
    "    lr = LinearRegression (featuresCol='features',labelCol='close',maxIter=10,regParam=0.3,elasticNetParam=0.7)\n",
    "    trained_model = lr.fit(output)\n",
    "    print(\"processing data for \"+name+\"...\")\n",
    "    # Performs the recursive prediction based on the trained_model that we have just created\n",
    "    predictions = recursive_prediction(df, trained_model)\n",
    "    print(\"...done\")\n",
    "    # If we have previously executed the batch process (this file), \n",
    "    # we have to overwrite the model with fresh data\n",
    "    lr.write().overwrite().save(\"/models/\"+name)\n",
    "    historical_data.append(format_data(output,\"close\").toPandas().to_dict(orient=\"records\"))\n",
    "    prediction_data.append(format_data(predictions.withColumnRenamed(\"close\",\"prediction\"),\"prediction\").toPandas().to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0825a-8e50-49ca-9c3d-a6dbc2a0d1b6",
   "metadata": {},
   "source": [
    "Eseguiamo un ciclo for sull'array di nomi dei settori. In esso, carichiamo su \"df\" i files contenuti nella cartella del settore su cui ci troviamo. Costruiamo un oggetto di tipo \"VectorAssembler\" che non restituirà altro che uno schema, il quale condenserà i dati contenuti nei campi \"open\", \"high\" e \"low\" in un unico vettore, chiamato features, che si passa legittimamente al modello per il machine learning.\n",
    "Passiamo questo assembler al modello, chiamando su di esso il metodo \"transform\" e passando il dataframe appena caricato come argomento. Selezioniamo, sull'oggetto risultante, le colonne \"features\", \"close\", \"tickerSymbol\" e \"timestamp\" e carichiamo questo oggetto sulla variabile \"output\".\n",
    "Applichiamo l'algoritmo di linear regression a questo dafaframe \"output\", dapprima ottenendo \"lr\", che non sarà altro che un oggetto contenente le impostazioni dei parametri di cui ha bisogno la linear regression, (nello specifico, il contenuto del vettore appena creato sarà alla base della previsione, il cui valore sarà riportato nella colonna \"close\", ed eseguiremo un massimo di 10 iterazioni nell'allenamento). Poi salveremo il modello a seguito del suo training in \"trained_model\", ottenuto dalla chiamata del metodo \"fit\" su \"lr\", passando come parametro \"output\".\n",
    "Abbiamo tutto ciò che ci occorre per chiamare la funzione \"rucursive_prediction\", a partire dal dataframe iniziale e dal modello appena allenato. La funzione ci restituirà il dataframe \"prediction\", che conterrà il valore \"close\" per gli 11 giorni previsti a partire dalla sliding window, che inzia dagli ultimi 12 giorni di dati storici. \n",
    "Per salvare il modello \"lr\" nell'apposita cartella contenente il model dello specifico settore. chiamiamo sulla sua variabile il metodo \"write\", poi \"overwrite\" e poi \"save\" con il path su cui salvarlo.\n",
    "Popoliamo adesso quei due array vuoti specificati all'inizio del codice, che conterranno oggetti di tipo dictionary (i quali elasticsearch potrà accettare), che nel primo caso saranno costituiti dalle n_uple del settore specifico corrispondenti alla colonna \"close\", nell'altro saranno corrispondenti alla colonna \"prediction\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee90338-84cd-445a-bbfb-064a9a1cd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "    print(\"sending index \"+names[i]+\" to elasticsearch...\")\n",
    "    save_and_send_data(\"prediction\",prediction_data,names[i],indexes[i])\n",
    "    save_and_send_data(\"historical\",historical_data,names[i],indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f00db-e5ee-40b2-bac6-65ec9e2e91b6",
   "metadata": {},
   "source": [
    "Per ognuno degli oggetti di tipo Elasticsearch nell'array indexes, inviamo la specifica di quale dei due indici, due per settore, vogliamo popolare, dell'oggetto di tipo dictionary, del nome del settore a cui è associato e del nome dell'oggetto elasticsearch a cui vogliamo inviare i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc681e4-8fc9-457c-a392-d930fd52bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(dataframe, close_name):\n",
    "    return dataframe.groupBy(\"timestamp\").avg(close_name).sort(\"timestamp\").withColumn(\"timestamp\",predictions.timestamp / 1000).withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType())).\\\n",
    "    withColumn(\"timestamp\",date_format(col(\"timestamp\"),\"yyyy-MM-dd\")).withColumn(\"avg(\"+close_name+\")\",round(col(\"avg(\"+close_name+\")\"),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98eff36-dff8-4697-aedf-fb0b5846c6f7",
   "metadata": {},
   "source": [
    "Lo scopo di questa funzione, chiamata nel momento in cui aggiungiamo agli array di oggetti dictionary, è di restituire un dataframe ben formattato da trasformare nel suddetto tipo. \n",
    "Raggruppiamo le sue n-uple per timestamp, eseguiamo una media aritmetica dei valori del campo close per l'uno e prediction per l'altro, ordiniamo gli ormai singoli valori (a seconda del settore) in modo decrescente, sostituiamo il timestamp (dovrebbe essere chiamato su dataframe, no?) ottenendolo in secondi e castiamo questi valori al formato \"anno, mese e giorno\". Dall'altra parte, rinominiamo la colonna che contiene i valori di close o di prediction in modo da contestualizzare il fatto che si tratti ora di una media di quei valori, dopodichè arrotondiamo il valore della media alla seconda cifra decimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f4a3c-1a6c-4c53-b6c2-a1fffc676571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_ms_to_weekday(epoch_time_in_ms):\n",
    "    dt_object = datetime.datetime.fromtimestamp(epoch_time_in_ms/1000)\n",
    "    # Get the weekday as an integer (Monday is 0 and Sunday is 6)\n",
    "    weekday_number = dt_object.weekday()\n",
    "    return weekday_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7186171-ea05-4366-a81b-21742752fa74",
   "metadata": {},
   "source": [
    "Lo scopo di questa funzione, come è apparente dal suo nome, è di trasformare il nostro timestamp (espresso in millisecondi) in un giorno preciso della settimana. Per fare questo, usiamo il metodo \"fromtimestamp\" della classe \"Datetime\", inserendo il timestamp in secondi come parametro. Salviamo l'oggetto restituito in \"dt_object\", poi chiamiamo su quest'ultimo oggetto il metodo \"weekday\" e sulla variabile \"weekday_number\" avremo il formato della data che volevamo, la quale restituiremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a03da-d9ea-48ea-b150-fec4c37578bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_prediction(dataframe, model, prediction=None, nIter=window_size):\n",
    "    max_t = int(dataframe.select(F.max(\"timestamp\")).first()[0])\n",
    "    curr_weekday = epoch_ms_to_weekday(max_t+day_in_ms)\n",
    "    day_offset = day_in_ms\n",
    "    if curr_weekday > 4:\n",
    "        day_offset = day_offset*(8-curr_weekday)\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"tickerSymbol\").orderBy(col(\"timestamp\").desc())\n",
    "    df_with_row_number = dataframe.withColumn(\"row_num\", row_number().over(window_spec)).select(\"tickerSymbol\",\"open\",\"high\",\"low\",\"timestamp\",\"close\")\n",
    "    result_df = df_with_row_number.filter(col(\"row_num\") < window_size).drop(\"row_num\")\n",
    "    temp_df = result_df.groupBy(\"tickerSymbol\").agg(F.avg(\"open\").alias(\"open\"),F.avg(\"high\").alias(\"high\"),F.avg(\"low\").alias(\"low\"),F.any_value(\"timestamp\").alias(\"timestamp\"),F.avg(\"close\").alias(\"close\"))\n",
    "    assembler = VectorAssembler(inputCols=['open','high','low'],outputCol='features')\n",
    "    temp_df = assembler.transform(temp_df)\n",
    "\n",
    "    if(prediction != None):\n",
    "        print(\"Completion \"+str(int(((window_size-nIter+1)/window_size)*100))+\"%\")\n",
    "        prediction = prediction.withColumn(\"timestamp\",lit(max_t+day_offset))\n",
    "        result_df = result_df.union(prediction.withColumnRenamed(\"prediction\",\"close\").select(\"tickerSymbol\",\"open\",\"high\",\"low\",\"timestamp\"\\\n",
    "        ,\"close\")).distinct()\n",
    "        nIter = nIter - 1\n",
    "    prediction = trained_model.transform(temp_df).select(\"tickerSymbol\",\"open\",\"high\",\"low\",\"timestamp\",\"prediction\",\"features\")\n",
    "    if nIter == 0 and result_df is not None:\n",
    "        return result_df\n",
    "    else:\n",
    "        return recursive_prediction(dataframe=result_df,model=trained_model,prediction=prediction,nIter=nIter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2e733-d37b-4265-833d-cfb36b7d92ec",
   "metadata": {},
   "source": [
    "Passiamo ora al cuore di questo codice, ovvero alla funzione al termine della quale avremo a disposizione il nostro dataframe *prediction* con tutti i valori della *close* predetti fino a un giorno in meno della sliding windows di valori storici e non su cui ci basiamo (importante ricordare che questi valori non siano quelli dei singoli giorni, ma le loro medie, in base al settore).\n",
    "\n",
    "Innanzitutto, la prima volta in cui la funzione ricorsiva sarà chiamata, ad essere passato sarà il dataframe originale, quello che viene fuori da tutti i file \"txt\" presenti nelle cartelle che indicano i settori. Come secondo parametro, passeremo il modello già allenato. Poi abbiamo due parametri di default, l'uno *prediction* che si riferisce al dataframe delle predizioni mano a mano risultante, l'altro *nIter*, inizialmente impostato al valore di *window_size*, ovvero l'arco di tempo della nostra sliding window.\n",
    "\n",
    "Il primo passo consiste nell'estrarre il timestamp con il valore più alto, dunque il valore del giorno più recente, e salvarlo in *max_t*. Per ottenere da questo il valore del giorno della settimana successivo, passiamo quel timestramp, sommato ai millisecondi che caratterizzano il trascorrere di un giorno, alla funzione *epoch_ms_to_weekday* già discussa. \n",
    "Dopo aver salvato un'unità di tempo (il giorno in millisecondi) su *day_offset*, verifichiamo se il giorno più recente sia maggiore del quarto giorno della settimana, ovvero oltre giovedì. Se è così, il *day_offset*, piuttosto che rimanere a un giorno, andrà a rappresentare il numero di giorni che servono per mappare opportunemente i giorni di fine settimana mancanti dai dati ai primi due giorni della settimana successiva.\n",
    "\n",
    "Su *window_spec* carichiamo un oggetto di tipo *Window* ovvero un oggetto recante le caratteristiche che una finestra dovrebbe assumere. Nello specifico, noi vogliamo che esista una finestra per ogni ticker, che contenga tutte le n-uple relative a quel ticker, ordinate per timestamp. Il nostro obiettivo iniziale è avere una numerazione delle n-uple, quindi dei giorni, in ordine decrescente di data. Infatti in \"df_with_row_numnber\", salviamo il dataframe che, a partire da quello originale, abbia anche la colonna *row_num*, basata sullo schema presente nell'oggetto *window_spec*, e per di più ad esclusione di tutti i campi non necessari alla nostra analisi. Infatti, comprenderemo solo *tickerSymbol*,*open*,*high*,*low*,*timestamp* e *close*.\n",
    "Su *result_df* avremo il dataframe che, sulla base del \"row_num\" del precedente, conterrà solo le n-uple relative agli ultimi tot giorni, in base alla window size, e che saranno prive del campo \"row_num\" stesso, ormai non utile per noi. \n",
    "\n",
    "A questo punto, creiamo un *temp_df* che, per ogni ticker, a partire delle tot n-uple, avrà una sola n-upla, indicante la media dei valori di ogni campo presente su *result_df*.\n",
    "Per procedere con l'applicazione del modello di machine learning, abbiamo bisogno che tutti questi valori in *temp_df* confluiscano all'interno di un vettore, corrispondente alla colonna \"features\". Dunque ora *temp_df* conterrà quella colonna con il vettore.\n",
    "A questo punto, se abbiamo passato alla funzione un dataframe \"prediction\" non nullo, salviamo sulla n-upla che lo caratterizza il timestamp opportuno in base al timestamp più recente presente in *max_t*, in modo che il valore sia considerato una stringa,\n",
    "Dopo ciò, aggiungiamo questa n-upla al nostro *result_df*, rinominando prima la colonna *prediction* (in prediction) in *close*, in modo che si possa operare una union tra *result_df* e *prediction* (quest'ultimo in quanto dataframe). Decrementiamo poi il numero di iterazioni della funzione.\n",
    "\n",
    "A prescindere dall'esistenza o meno di *prediction*, creiamolo o reinizializziamolo perchè contenga la predizione sulla giornata immediatamente successiva, basata sulle medie presenti in *temp_df*. Lo facciamo chiamando in causa il modello già allenato e passandogli, appunto, *temp_df*.\n",
    "\n",
    "Data la ricorsività della funzione, ci occorre identificare un caso base e prevedere che operazioni compiere nell'ambito di esso e poi del passo ricorsivo. \n",
    "Il nostro caso base occorre quando il numero di iterazioni è pari a 0 e *result_df* non è nullo. Dunque, restituiamo *result_df*, che non sarà altro che il dataframe finale, contenente, oltre i dati storici, un giorno in meno di predizioni rispetto alla sliding window prevista.\n",
    "Il passo ricorsivo viene eseguito altrimenti, richiamando recursive_prediction passando l'attuale *result_df* quale *dataframe*, il *trained_model* quale *model*, il dataframe *prediction* quale *prediction* e l'attuale *nIter* quale *nIter*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39db283",
   "metadata": {},
   "source": [
    "**STREAMING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad01a42",
   "metadata": {},
   "source": [
    "La fase streaming è la seconda ed ultima fase del progetto. Essa consente la visualizzazione dei dati prodotti dalla prima fase, che saranno sovrapposti ad un flusso di dati in streaming relativi alla giornata di oggi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6dbf4",
   "metadata": {},
   "source": [
    "CLIENT PHP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e192d5e",
   "metadata": {},
   "source": [
    "Similarmente alla fase batch, ci avvaliamo di un client PHP per la richiesta dei dati all'API. Le differenze principali saranno discusse di seguito;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf76d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "$tickers = array(\n",
    "    \"AXP\" => \"financial\",\n",
    "    \"AMGN\" => \"health\",\n",
    "    \"AAPL\" => \"tech\",\n",
    "    \"BA\" => \"industrial\",\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    ");\n",
    "\n",
    "$currentDate = new DateTime();\n",
    "$startingDate = new DateTime();\n",
    "$interval = new DateInterval('P2D');\n",
    "$startingDate->sub($interval);\n",
    "\n",
    "$rest = new Rest($api_key);\n",
    "\n",
    "function write_batch($data_array, $ticker, $category, $tickers)\n",
    "{\n",
    "    $path = \"/data/raw/$category\";\n",
    "    $filename = \"$ticker.txt\";\n",
    "    if (!file_exists($path))\n",
    "        mkdir($path);\n",
    "    $file = fopen($path . \"/\" . $filename, \"w\") or die(\"Unable to open file !\");\n",
    "    $results = $data_array[\"results\"];\n",
    "    foreach ($results as $result) {\n",
    "        usleep(250000);\n",
    "        $result[\"tickerSymbol\"] = $ticker;\n",
    "        fwrite($file, json_encode($result) . \"\\n\");\n",
    "    }\n",
    "    fclose($file);\n",
    "}\n",
    "\n",
    "while (true) {\n",
    "    foreach ($tickers as $ticker => $category) {\n",
    "        $data = $rest->stocks->aggregates->get(\n",
    "            $ticker,\n",
    "            15,\n",
    "            $startingDate->format('Y-m-d'),\n",
    "            $currentDate->format('Y-m-d'),\n",
    "            'minute'\n",
    "        );\n",
    "        sleep(1);\n",
    "        // var_dump($data);\n",
    "        write_batch($data, $ticker, $category, $tickers);\n",
    "    }\n",
    "    sleep(60*15);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fe4c3",
   "metadata": {},
   "source": [
    "La richiesta dei dati viene fatta su un arco temporale molto più ristretto, al più di due giorni. Tale richiesta verrà riproposta ogni 15 minuti, ragione per il quale effettuiamo una sleep esattamente di tale valore.\n",
    "\n",
    "Nel momento dell'apertura della borsa, l'API sarà in grado di fornirci i dati del giorno attuale, che ci consentiranno di visualizzare in tempo quasi reale l'andamento del mercato in relazione al settore scelto. Per tale motivo la granularità dei dati arrivatici è di 15 minuti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00987e76",
   "metadata": {},
   "source": [
    "*DATA INGESTION*\n",
    "\n",
    "Abbiamo scelto di utilizzare Fluentd come framework di DI. Esso permette \n",
    "con facilità di accedere a files salvati nel proprio file system e usarli \n",
    "quali source.\n",
    "\n",
    "*Dockerfile*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM fluentd\n",
    "USER root\n",
    "RUN apk add ruby-dev\n",
    "RUN gem install fluent-plugin-kafka --no-doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684b306",
   "metadata": {},
   "source": [
    "Partiamo dall'immagine \"vergine\" di Fluentd, scaricata direttamente dalla\n",
    "repository pubblica, per poi installare ruby e le sue relative librerie di header per la compilazione di pacchetti relativi a ruby, e tramite il gestore di pacchetti di Ruby installiamo il plugin che permette lo scambio di dati tra fluentd e kafka. L'immagine risultante sarà chiamata \"fluentkafka\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018d0cc",
   "metadata": {},
   "source": [
    "*fluent.conf*\n",
    "\n",
    "La tappa più importante, dopo aver creato opportunemente il\n",
    "Dockerfile e dunque aver avuto a disposizione l'immagine per il container,\n",
    "è quella di scrivere correttamente il file di configurazione. \n",
    "Esistono due tag fondamentali che lo caratterizzano, ovvero \"\\<source\\>\" e \n",
    "\"\\<match\\>\", rispettivamente a indicare l'input e l'output della data \n",
    "ingestion. \n",
    "Source.\n",
    "\n",
    "L'annotazione \"@type tail\" si utilizza per indicare che si intenda aprire\n",
    "un file. Questi vengono letti, come suggerisce il nome stesso, dall'ultima\n",
    "riga alla prima a meno che, come abbiamo fatto, non si setti la variabile\n",
    "read_from_head a \"true\".  \n",
    "I files da cui stiamo leggendo, come già spiegato nella sezione CLIENT PHP,\n",
    "sono caratterizzati da un JSON per ogni riga. Dunque, indicheremo \"format\n",
    "JSON\" affinchè vengano riconosciuti da fluentd come tali.\n",
    "Visto che abbiamo scelto di suddividere le aziende in base al settore,\n",
    "continuiamo a mantenere questi gruppi specificando una source per ognuno\n",
    "di essi. Qualora non li taggassimo opportunemente, non riusciremmo ad \n",
    "utilizzarli in modo specifico all'interno del match. Per cui, per ognuno\n",
    "di essi, scriveremo \"tag\" seguito dal nome del settore.\n",
    "Infine, sarà fondamentale indicare il path da cui prelevarli, così come\n",
    "un path temporaneo, su \"pos_file\", che non indicherà altro che la cartella\n",
    "temporanea in cui si immagazzineranno i dati prima del loro invio in output.\n",
    "Match.\n",
    "\n",
    "L'annotazione \"@type kafka2\" indicherà che l'output verrà reindirizzato a \n",
    "kafka. Ciò è possibile in virtù dell'installazione del plugin che connette\n",
    "fluentd a kafka, da Dockerfile. \n",
    "Il suddetto plugin ci permetterà di specificare due unità fondamentali per\n",
    "kafka, ovvero i brokers e i topics. Abbiamo scelto, per leggibilità, di \n",
    "chiamare ogni broker come \"k-[nomebroker]\" e di bindarlo con la porta\n",
    "\"9092\", la quale sarà mappata a una porta effettiva al di fuori del container\n",
    "dell'istanza di kafka, che coincide con il broker stesso \n",
    "(si veda il docker_compose). Come nome del topic, avremo lo stesso nome\n",
    "del tag specificato in \"\\<source\\>\". Avremo quindi un \"\\<match\\>\" per\n",
    "ogni \"\\<source\\>\". Per specificare quali dati in input inoltreremo, il tag\n",
    "sarà \"\\<match [nometag]\\>\".\n",
    "Risulta necessario specificare anche il tipo di file in output, attraverso il\n",
    "tag \"@type JSON\" all'interno del tag \"\\<format\\>\", rigorosamente annidato dentro\n",
    "il match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d6343",
   "metadata": {},
   "source": [
    "*DATA DISTRIBUTION*\n",
    "\n",
    "Al fine di smistare opportunemente i dati acquisiti, abbiamo usato Apache Kafka. Le unità fondamentali tipiche di questo servizio sono i brokers, i topics, i producers e i consumers. La nostra scelta è stata di creare, come detto, 6 topic, forniti da altrettanti brokers. Ai fini della pipeline, non è necessario istruire producers e consumers (per debug, così da accertarci che i dati effettivamente fossero inviati da fluentd, abbiamo passato argomenti al file \"sh\" del consumer affinchè stampasse su console il contenuto di questi JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af857b3a",
   "metadata": {},
   "source": [
    "*Dockerfile*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM bitnami/kafka\n",
    "RUN rm -f /bitnami/kafka/data/.lock\n",
    "COPY init.sh /opt/bitnami/scripts/kafka\n",
    "COPY init /opt/bitnami/scripts/kafka\n",
    "CMD [\"/opt/bitnami/scripts/kafka/init\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8017c",
   "metadata": {},
   "source": [
    "Ognuna della 6 istanze di kafka, identificate da un broker per ciascuna, si basa su questa immagine. Dopo aver scaricato l'immagine ufficiale di kafka, si aggiunge lo script \"init.sh\" all'interno del file system del container, così come \"init\" (il file compilato basato sul codice in c che ci permette un'opportuna temporizzazione tra il deployment del server e la creazione dei topics, rigorosamente l'una dopo l'altra, eseguendo prima lo script run.sh e poi init.sh) e si esegue \"init\".\n",
    "L'immagine risultante sarà chiamata \"kafka_init\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb13342",
   "metadata": {},
   "source": [
    "*Zookeeper e ZooNavigator*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f61dd6",
   "metadata": {},
   "source": [
    "Per navigare agilmente tra l'ampia offerta di servizi di Kafka, è sempre necessario che con i server kafka si attivi anche un server zookeeper e un altro server, nel nostro caso \"zoonavigator\", che consenta l'accesso via browser dell'interfaccia grafica di questo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102bdb81",
   "metadata": {},
   "source": [
    "*Dockerfile*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM zookeeper\n",
    "RUN /apache-zookeeper-3.9.1-bin/bin/zkCli.sh << \"delete /brokers/ids/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb719a86",
   "metadata": {},
   "source": [
    "Questo Dockerfile parte dall'immagine ufficiale di zookeeper. L'immagine che costituisce sarà chiamata \"zookeeper_clean\"\n",
    "\n",
    "Lo scopo di zookeeper_clean è di assicurarsi che non vi siano broker già presenti all'avvio, richiamando così la funzione di cancellazione offerta dall'apposito script integrato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9331a",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba92a0",
   "metadata": {},
   "source": [
    "process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    \"cgoods\":0,\n",
    "    \"financial\":1,\n",
    "    \"energy\":2,\n",
    "    \"health\":3,\n",
    "    \"industrial\":4,\n",
    "    \"tech\":5\n",
    "}\n",
    "\n",
    "number_of_items = {\n",
    "    \"cgoods\":5,\n",
    "    \"financial\":5,\n",
    "    \"energy\":1,\n",
    "    \"health\":6,\n",
    "    \"industrial\":5,\n",
    "    \"tech\":8\n",
    "}\n",
    "\n",
    "# Definire lo schema dei dati\n",
    "schema = StructType([\n",
    "    StructField(\"v\", LongType(), True),\n",
    "    StructField(\"vw\", DoubleType(), True),\n",
    "    StructField(\"o\", DoubleType(), True),\n",
    "    StructField(\"c\", DoubleType(), True),\n",
    "    StructField(\"h\", DoubleType(), True),\n",
    "    StructField(\"l\", DoubleType(), True),\n",
    "    StructField(\"t\", LongType(), True),\n",
    "    StructField(\"n\", LongType(), True),\n",
    "    StructField(\"tickerSymbol\", StringType(), True),\n",
    "    StructField(\"volume\", LongType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"numberOfItems\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Definire i topic di Kafka da cui leggere i dati\n",
    "names=[\"cgoods\",\"financial\",\"energy\",\"health\",\"industrial\",\"tech\"]\n",
    "\n",
    "# Creare un DataFrame per ogni topic\n",
    "dataframes = [0,1,2,3,4,5]\n",
    "averages = [1,2,3,4,5,6]\n",
    "threads = []\n",
    "historical_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdf10e",
   "metadata": {},
   "source": [
    "E' importante inizializzare una serie di strutture dati. Si parte con un array associativo \"topics\", che correla la chiave (il nome del topic) con un valore da 0 a 5. Ci servirà per indirizzare opportunemente operazioni compiute su altre strutture che vedremo, sulla base del nome del topic. L'altro array associativo \"number_of_items\" correla i topics con il numero di aziende che ne fanno parte. Lo \"schema\" contiene le istruzioni per strutturare opportunamente la singola n-upla. Invece \"dataframes\" è un array che per ora inizializziamo con gli interi e successivamente re-inizializziamo, come vedremo. Stesso varrà per \"averages\", mentre \"threads\" contiene i riferimenti ai 6 threads che creeremo, sempre uno per settore. Infine \"historical_data\" conterrà oggetti di tipo dictionary che potremo inviare ad elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcb8a8-7e16-4089-8033-cc57c3e600d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    thread = threading.Thread(target=writeKafkaStreamingData, args=(dataframes,names[i]))\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "time.sleep(15) # Diamo tempo ai container di avviarsi\n",
    "\n",
    "load_indexes()\n",
    "\n",
    "for i in range(6):\n",
    "    averages[i] = dataframes[i].select(col(\"data.close\"),col(\"data.timestamp\"),col(\"topic\"))\n",
    "    # averages[i].printSchema()\n",
    "    averages[i] = averages[i].groupBy(\"timestamp\").avg(\"close\").sort(\"timestamp\").withColumn(\"timestamp\",averages[i].timestamp / 1000).withColumn(\"timestamp\", date_format(col(\"timestamp\").cast(TimestampType()), \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\")).withColumn(\"avg(close)\",round(col(\"avg(close)\"),2))\n",
    "    averages[i] = averages[i].writeStream.outputMode(\"complete\").queryName(names[i]).format(\"memory\").start()\n",
    "while(True):\n",
    "    for i in range(6):\n",
    "        temp_sdf = spark.sql(\"SELECT * FROM \"+names[i])\n",
    "        if temp_sdf.count() > 0:\n",
    "            historical_data.append(temp_sdf.toPandas().to_dict(orient=\"records\"))\n",
    "            save_and_send_data(historical_data,names[i],indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d0995-c900-4212-a58a-f835d0f81325",
   "metadata": {},
   "source": [
    "Il primo ciclo for, come detto, origina i threads, specificando per ognuuno quale sia la funzione che debbano eseguire e gli argomenti di cui quella funzione necessiti. Ogni nuova istanza si aggiunge all'array \"threads\". \n",
    "Il secondo ciclo for, per ogni \"thread\", lo starta. Si esegue poi una sleep che consenta a tutti gli altri containers, utili all'esecuzione dei passi precedenti della pipeline, di essere eseguiti.\n",
    "Dopo aver chiamato la funzione \"load_indexes\" si esegue un altro ciclo for, alla fine del quale ognuna delle posizioni dell'array \"averages\" viene inizializzata con i dataframes che conterranno i dati pervenutici dallo streaming (non sono dei veri e propri dataframes, ma degli oggetti di tipo \"streamingQuery\", atti a mantenere lo streaming attivo in modo da scrivere su delle tabelle contenute nell'oggetto \"spark\", poi accessibili attraverso codice sql). Questi contengono due colonne, l'una quella del timestamp (la cui data sarà visualizzata nel formato standard, così come l'ora) e l'altra, \"avg(close\", quella del valore medio a chiusura (risultato della media aritmetica di tutti i prezzi a chiusura delle aziende dello specifico settore) rispetto al timestamp.\n",
    "Nell'ultimo ciclo infinito, scelto proprio perchè è previsto che i dati continuino ad arrivare, si esegue un ciclo for in cui, per ogni topic, si inizializza un dataframe chiamato \"temp_sdf\" con lo specifico dataframe delle medie (come abbiamo detto, accedendo all'oggetto \"spark\" attraverso la funzione sql e referenziando la tabella del topic). Se questo dataframe contiene almeno una n-upla, lo si aggiunge all'array \"historical_data\" attraverso la funzione \"append\", come oggetto di tipo \"dictionary\". Dopodichè si utilizza la solita funzione \"save_and_send_data\" per inviare questi oggetti a elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7853e-4acf-45bc-b69b-9e4eb5c57608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeKafkaStreamingData(dataframes,topic):\n",
    "    print(\"Executing \"+topic+\" thread...\")\n",
    "    dataframes[topics[topic]] = subscribeToTopic(dataframes,topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbc194-f173-405c-832e-b16e2022fac0",
   "metadata": {},
   "source": [
    "Questa funzione, che verrà eseguita dal singolo thread, inizializza l'array \"dataframes\" con i dataframes ottenuti dopo le sottoscrizioni ai topics di kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09d271-ed7b-4679-ba8b-6546ec3ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subscribeToTopic(dataframes,topic):\n",
    "    df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"k-cgoods:9092,k-energy:9092,k-financial:9092,k-health:9092,k-industrial:9092,k-tech:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load().select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"),col(\"topic\").cast(\"string\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d49e2-6b58-48f6-8a95-f36420ddb82d",
   "metadata": {},
   "source": [
    "Quì ci iscriviamo (inizializzando un nuovo dataframe, che poi restituiremo) usufruendo del plugin che collega kafka a spark, specificando da un lato quali siano i brokers per i quali ci terremo in ascolto, dall'altro il singolo topic a cui uno di questi broker si riferisce. Il dataframe sarà ricavato sulla base del json costituente i dati presenti nel topic, il quale li contiene nell'attributo \"value\". Dopo aver effettuato il cast a \"string\" e applicato su di essi lo \"schema\", dunque dopo averli predisposti a rappresentare un dataframe vero e proprio, rinominiamo la colonna \"value\" con il nome \"data\". Inoltre, castiamo la colonna \"topic\" a \"string\" e selezioniamo sia questa che la precedente in modo che il dataframe finale contenga solo il contenuto di entrambe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdda21f-3510-4443-b729-64d2c26fdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indexes():\n",
    "    for name in names: # Ricostituiamo i dati storici + previsioni fornite dal lato batch\n",
    "        historical = open(\"/indexes/\"+name+\"_historical.txt\")\n",
    "        prediction = open(\"/indexes/\"+name+\"_prediction.txt\")\n",
    "        for line in historical:\n",
    "            indexes[topics[name]].index(index=name+\"_historical\",document=json.loads(line),timeout=\"30s\") \n",
    "        for line in prediction:\n",
    "            indexes[topics[name]].index(index=name+\"_prediction\",document=json.loads(line),timeout=\"30s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f8bbc-ef92-41cf-a022-03d4ffd316e0",
   "metadata": {},
   "source": [
    "Questa funzione ha il solo scopo di riportare a elasticsearch tutti gli indici già ottenuti con l'esecuzione del batch, in modo da non dover rieseguire quest'ultimo contestualmente. Gli oggetti \"historical\" e \"prediction\", a partire da file di tipo \"txt\" caratterizzanti gli indici (presenti nel nostro file system), si possono scorrere linea per linea, così da ridiventare oggetti di tipo dictionary, inviabili ad elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7683c-ebb9-4a62-a3c0-2600d364baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_send_data(data, name, es_index):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            json_dump = json.dumps(data[i][j])\n",
    "            es_index.index(index=name+\"_streaming\", id=j,document=json_dump)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd49da-0a99-4c69-afd4-e6bcaea1c0be",
   "metadata": {},
   "source": [
    "In questo doppio ciclo for, a partire da \"data\", quale array di oggetti di tipo dictionary (ovvero \"historical_data\"), dal \"name\" (che sarà il nome del topic) e dall'oggetto di tipo elasticsearch (che ci permette la creazione di un indice dentro la specifica istanza, come abbiamo già visto nella parte del batch), si scorrono tutti i dataframes dello streaming (il loro numero è la lunghezza di \"data\") e, per ognuno di loro, si crea un oggetto json, il quale conterrà, per ogni settore, le n-uple che caratterizzano le medie dei prezzi a chiusura in base al timestamp (infatti la lunghezza di \"data[i]\" non è altro che il numero di n-uple che caratterizza il singolo dataframe a cui si accede)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
