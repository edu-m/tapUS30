{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581dc5c-450a-4752-afca-e4f195d4972e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "*TAP US30*\n",
    "\n",
    "STREAMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f0a3b-a4ca-484c-92f6-3366565f7555",
   "metadata": {},
   "source": [
    "CLIENT PHP\n",
    "\n",
    "Il primo passo della pipeline è quello di creare un client che s'interfacci\n",
    "con l'API di Polygon e faccia richiesta dei dati, considerando la sola giornata \n",
    "precedente. Quest'ultimo sarà mandato in esecuzione su un container.Di seguito, \n",
    "vediamo come ottenere l'opportuno Dockerfile, da cui potremo creare l'immagine \n",
    "che, nel nostro \"docker-compose\", chiameremo \"producer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942c313-2c6b-406a-8b42-2acd2d36671c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FROM alpine\n",
    "RUN apk update && apk add php composer php-fileinfo\n",
    "WORKDIR /app\n",
    "COPY batch_extract.php /app\n",
    "RUN composer require polygon-io/api -W\n",
    "CMD [\"php\", \"batch_extract.php\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969dc170-6bb6-4264-9177-7f414cb8c01a",
   "metadata": {},
   "source": [
    "L'immagine base è quella della versione \"alpine\" di Linux, scelta in quanto\n",
    "tra le più \"leggere\" in termini di spazio sul disco. Il gestore di pacchetti\n",
    "si chiama \"apk\", da cui installiamo le librerie php e composer. La directory \"/app\" nel\n",
    "container è quella in cui copiamo il codice \"batch_extract.php\", il cui\n",
    "contenuto descriveremo in seguito. Eseguiamo il comando \"composer\" per richiedere\n",
    "la libreria da utilizzare nel nostro codice php così da accedere all'API.\n",
    "Dopodichè, una volta partito il container, viene eseguto il comando che\n",
    "innesca l'esecuzione del suddetto codice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79a18e-868e-4eb6-b8ed-acaa7a8e1017",
   "metadata": {},
   "source": [
    "*batch_extract.php*\n",
    "\n",
    "Innanzitutto, salviamo la API key in una opportuna variabile. Creaiamo un array\n",
    "associativo con come chiave il nome dell'azienda e come valore il settore in cui\n",
    "opera. Dichiariamo poi un oggetto di tipo \"Rest\" sul quale chiameremo una serie di\n",
    "funzioni per prelevare i dati che ci occorrono. Il più importante è il metodo \"get\"\n",
    "che richiede come parametri il nome dell'azienda, un intero, la data di inizio e \n",
    "quella di fine entro cui prelevare le informazioni e l'unità di tempo, che indichiamo\n",
    "come il singolo giorno. In un ciclo for avanzato, scorriamo l'array associativo. ogni\n",
    "12 secondi, inviamo una richiesta attraverso l'API. Salviamo queste informazioni su una\n",
    "variabile che viene passata alla nostra funzione \"write_batch\", insieme al nome \n",
    "dell'azienda e alla categoria a cui appartiene, aggiungendo poi anche l'array\n",
    "associativo stesso. \n",
    "Dopo aver proceduto, alla creazione della cartella e del file, si salva in una variabile\n",
    "l'array alla posizione \"results\" dell'array ottenuto dalla get. Si effettua un ciclo su\n",
    "tutte le n-uple che caratterizzano i dati della specifica azienda, a partire dall'array\n",
    "salvato nella variabile e, dopo aver aggiunto il ticker symbol in questione (dato che \n",
    "altrimenti non sarebbe presente per ogni n-upla, il che creerebbe dei problemi per il \n",
    "successivo processing), si scrive sul file appena creato il contenuto dell'array, \n",
    "sottoforma di json. Fuori dal ciclo, si procede alla chiusura del file.\n",
    "Quello che otterremo è quindi la presenza di 6 cartelle, una per ogni settore a cui le\n",
    "30 aziende appartengono, e ognuna di queste cartelle avrà dei file con estensione \"txt\" \n",
    "composti da n j-son, uno per ogni giorno in cui ogni azienda opera nel mercato azionario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86c71d-b5c2-446a-aae6-9618617c4bb9",
   "metadata": {},
   "source": [
    "*DATA INGESTION*\n",
    "\n",
    "Innanzitutto, abbiamo utilizzato Fluentd come framework. Esso permette \n",
    "con facilità di accedere a files salvati nel proprio file system e usarli \n",
    "quali source.\n",
    "Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf70d3c-f6c1-457e-9cb3-b66c9c7c35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM fluentd\n",
    "USER root\n",
    "RUN apk add ruby-dev\n",
    "RUN gem install fluent-plugin-kafka --no-doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb14a1-8155-4641-b7db-75d803376b69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Partiamo dall'immagine \"vergine\" di fluentd, scaricata direttamente dalla\n",
    "repository pubblica, per poi installare ruby e gem, quest'ultimo il gestore\n",
    "di pacchetti opportuno per installare il plugin che permette lo scambio di\n",
    "dati tra fluentd e kafka. L'immagine risultante sarà chiamata \"fluentkafka\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c953a-62c2-458e-ac4f-ef2934ffcbe2",
   "metadata": {},
   "source": [
    "*fluent.conf*\n",
    "\n",
    "La tappa più importante, dopo aver creato opportunemente il\n",
    "Dockerfile e dunque aver avuto a disposizione l'immagine per il container,\n",
    "è quella di scrivere correttamente il file di configurazione. \n",
    "Esistono due tag fondamentali che lo caratterizzano, ovvero \"<source>\" e \n",
    "\"<match>\", rispettivamente a indicare l'input e l'output della data \n",
    "ingestion. \n",
    "Source.\n",
    "\n",
    "L'annotazione \"@type tail\" si utilizza per indicare che si intenda aprire\n",
    "un file. Questi vengono letti, come suggerisce il nome stesso, dall'ultima\n",
    "riga alla prima a meno che, come abbiamo fatto, non si setti la variabile\n",
    "read_from_head a \"true\".  \n",
    "I files da cui stiamo leggendo, come già spiegato nella sezione CLIENT PHP,\n",
    "sono caratterizzati da un json per ogni riga. Dunque, indicheremo \"format\n",
    "json\" affinchè vengano riconosciuti da fluentd come tali.\n",
    "Visto che abbiamo scelto di suddividere le aziende in base al settore,\n",
    "continuiamo a mantenere questi gruppi specificando una source per ognuno\n",
    "di essi. Qualora non li taggassimo opportunemente, non riusciremmo ad \n",
    "utilizzarli in modo specifico all'interno del match. Per cui, per ognuno\n",
    "di essi, scriveremo \"tag\" seguito dal nome del settore.\n",
    "Infine, sarà fondamentale indicare il path da cui prelevarli, così come\n",
    "un path temporaneo, su \"pos_file\", che non indicherà altro che la cartella\n",
    "temporanea in cui si immagazzineranno i dati prima del loro invio in output.\n",
    "Match.\n",
    "\n",
    "L'annotazione \"@type kafka2\" indicherà che l'output verrà reindirizzato a \n",
    "kafka. Ciò è possibile in virtù dell'installazione del plugin che connette\n",
    "fluentd a kafka, da Dockerfile. \n",
    "Il suddetto plugin ci permetterà di specificare due unità fondamentali per\n",
    "kafka, ovvero i brokers e i topics. Abbiamo scelto, per leggibilità, di \n",
    "chiamare ogni broker come \"k-[nomebroker]\" e di bindarlo con la porta\n",
    "\"9092\", la quale sarà mappata a una porta effettiva al di fuori del container\n",
    "dell'istanza di kafka, che coincide con il broker stesso \n",
    "(si veda il docker_compose). Come nome del topic, avremo lo stesso nome\n",
    "del tag specificato in \"<source>\". Avremo quindi un \"<match>\" per\n",
    "ogni \"<source>\". Per specificare quali dati in input inoltreremo, il tag\n",
    "sarà \"<match [nometag]>\".\n",
    "Risulta necessario specificare anche il tipo di file in output, attraverso il\n",
    "tag \"@type json\" all'interno del tag \"<format>\", rigorosamente annidato dentro\n",
    "il match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65642749-7148-4be0-bbb8-f873b56a712c",
   "metadata": {},
   "source": [
    "DATA DISTRIBUTION\n",
    "Al fine di smistare opportunemente i dati acquisiti, abbiamo usato Apache Kafka. Le unità fondamentali tipiche di questo servizio sono i brokers, i topics, i producers e i consumers. La nostra scelta è stata di creare, come detto, 6 topic, forniti da altrettanti brokers. Ai fini della pipeline, non è necessario istruire producers e consumers (per debug, così da accertarci che i dati effettivamente fossero inviati da fluentd, abbiamo passato argomenti al file \"sh\" del consumer affinchè stampasse su console il contenuto di questi json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606538b7-450c-4d5e-b871-d45a4af3436b",
   "metadata": {},
   "source": [
    "Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13337bc0-7513-4e44-9255-600a674c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM bitnami/kafka\n",
    "RUN rm -f /bitnami/kafka/data/.lock\n",
    "COPY init.sh /opt/bitnami/scripts/kafka\n",
    "COPY init /opt/bitnami/scripts/kafka\n",
    "CMD [\"/opt/bitnami/scripts/kafka/init\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b087f-d60a-43a5-81e4-22956757856b",
   "metadata": {},
   "source": [
    "Ognuna della 6 istanze di kafka, identificate da un broker per ciascuna, si basa su questa immagine. Dopo aver scaricato l'immagine ufficiale di kafka, si aggiunge lo script \"init.sh\" all'interno del file system del container, così come \"init\" (il file compilato basato sul codice in c che ci permette un'opportuna temporizzazione tra il deployment del server e la creazione dei topics, rigorosamente l'una dopo l'altra, eseguendo prima lo script run.sh e poi init.sh) e si esegue \"init\".\n",
    "L'immagine risultante sarà chiamata \"kafka_init\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cd61e-34f3-44a0-895b-2c8fd81dc827",
   "metadata": {},
   "source": [
    "Zookeeper e ZooNavigator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1059c-bdbe-437e-87a6-edcb1b71e0c5",
   "metadata": {},
   "source": [
    "Per navigare agilmente tra l'amplia offerta di servizi di Kafka, è sempre necessario che con i server kafka si attivi anche un server zookeeper e un altro server, nel nostro caso \"zoonavigator\", che consenta l'accesso via browser dell'interfaccia grafica di zookeeper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25828d-617d-46d3-96ac-3e0e70cd8e52",
   "metadata": {},
   "source": [
    "Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd6090-5fd2-4e0c-9ab3-d3f815e52d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM zookeeper\n",
    "RUN /apache-zookeeper-3.9.1-bin/bin/zkCli.sh << \"delete /brokers/ids/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b88b96-3bc8-44b2-a820-1a34046dad4e",
   "metadata": {},
   "source": [
    "Questo Dockerfile parte dall'immagine ufficiale di zookeeper. L'immagine che costituisce sarà chiamata \"zookeeper_clean\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a64cc-e122-4f80-bc04-c4b954dca877",
   "metadata": {},
   "source": [
    "DATA PROCESSING (BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549143b8-dc9b-48f7-9cc8-9ff559fb7f2d",
   "metadata": {},
   "source": [
    "process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a04a1-fb21-409d-966c-58e3b68c2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\",\"15g\")\\\n",
    "    .appName(\"tapus30\").getOrCreate()\n",
    "# spark = SparkSession(sc)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "names=[\"cgoods\",\"financial\",\"energy\",\"health\",\"industrial\",\"tech\"]\n",
    "indexes=[Elasticsearch(\"http://es_cgoods:9200\"),Elasticsearch(\"http://es_financial:9200\"),Elasticsearch(\"http://es_energy:9200\"),Elasticsearch(\"http://es_health:9200\"),Elasticsearch(\"http://es_industrial:9200\"),Elasticsearch(\"http://es_tech:9200\")]\n",
    "prediction_data=[]\n",
    "historical_data=[]\n",
    "day_in_ms = 86400000\n",
    "window_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405700e-73d6-4b46-a3c9-9a56488ff4b0",
   "metadata": {},
   "source": [
    "Innanzitutto, apriamo una sessione di Spark, scegliendo come master il nostro stesso container, garantendoci poi che renda disponibili un massimo di 15 gigabyte di ram per l'esecuzione del driver che stiamo scrivendo. Specifichiamo il nome del nostro driver e salviamo le impostazioni sull'oggetto \"spark\". Su questo verra richiamato \"sparkContext\" per l'apertura effettiva del context e salviamo ciò che ci viene restituito sulla variabile \"sc\", su cui chiameremo poi il metodo \"setLogLevel\", per decidere il tipo di logs che vogliamo siano prodotti all'esecuzione del codice.\n",
    "Creiamo poi un array con i nomi dei settori, che saranno anche i nomi degli indici su elasticsearch. Dichiariamo un array di oggetti Elasticsearch attraverso cui, successivamente, potremo effettivamente inviare i dati alle sue sei diverse istanze, ancora una volta una per settore. \n",
    "Dichiariamo poi due array vuoti, che saranno popolati dai dati restituitici dal modello di machine learning che useremo (prediction_data) e dai dati che preleveremo dal nostro file system, relativi ai 2 anni di operazioni in borsa (historical_data). \n",
    "Impostiamo un offset fondamentale per operare con i timestamp (ovvero le date espresse in millisecondi, a partire dall'omonimo campo all'interno dei json), cioè quello che ci consente di aggiungere un giorno alla data da cui partiamo. Altra importante variabile è quella \"window_size\", che ci permetterà di eseguire la funzione ricorsiva che vedremo per implementare una sliding window di 12 giorni, base su cui il modello via via restituirà l'ultima previsione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dd518-fc8b-4424-90c3-1afbf480f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    df = spark.read.json(\"/data/\"+name)\n",
    "    assembler = VectorAssembler(inputCols=['open','high','low'],outputCol='features')\n",
    "    output = assembler.transform(df).select('features','close','tickerSymbol','timestamp')\n",
    "    lr = LinearRegression (featuresCol='features',labelCol='close',maxIter=10,regParam=0.3,elasticNetParam=0.7)\n",
    "    trained_model = lr.fit(output)\n",
    "    print(\"processing data for \"+name+\"...\")\n",
    "    # Performs the recursive prediction based on the trained_model that we have just created\n",
    "    predictions = recursive_prediction(df, trained_model)\n",
    "    print(\"...done\")\n",
    "    # If we have previously executed the batch process (this file), \n",
    "    # we have to overwrite the model with fresh data\n",
    "    lr.write().overwrite().save(\"/models/\"+name)\n",
    "    historical_data.append(format_data(output,\"close\").toPandas().to_dict(orient=\"records\"))\n",
    "    prediction_data.append(format_data(predictions.withColumnRenamed(\"close\",\"prediction\"),\"prediction\").toPandas().to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0825a-8e50-49ca-9c3d-a6dbc2a0d1b6",
   "metadata": {},
   "source": [
    "Eseguiamo un ciclo for sull'array di nomi dei settori. In esso, carichiamo su \"df\" i files contenuti nella cartella del settore su cui ci troviamo. Costruiamo un oggetto di tipo \"VectorAssembler\" che non restituirà altro che uno schema, il quale condenserà i dati contenuti nei campi \"open\", \"high\" e \"low\" in un unico vettore, chiamato features, che si passa legittimamente al modello per il machine learning.\n",
    "Passiamo questo assembler al modello, chiamando su di esso il metodo \"transform\" e passando il dataframe appena caricato come argomento. Selezioniamo, sull'oggetto risultante, le colonne \"features\", \"close\", \"tickerSymbol\" e \"timestamp\" e carichiamo questo oggetto sulla variabile \"output\".\n",
    "Applichiamo l'algoritmo di linear regression a questo dafaframe \"output\", dapprima ottenendo \"lr\", che non sarà altro che un oggetto contenente le impostazioni dei parametri di cui ha bisogno la linear regression, (nello specifico, il contenuto del vettore appena creato sarà alla base della previsione, il cui valore sarà riportato nella colonna \"close\", ed eseguiremo un massimo di 10 iterazioni nell'allenamento). Poi salveremo il modello a seguito del suo training in \"trained_model\", ottenuto dalla chiamata del metodo \"fit\" su \"lr\", passando come parametro \"output\".\n",
    "Abbiamo tutto ciò che ci occorre per chiamare la funzione \"rucursive_prediction\", a partire dal dataframe iniziale e dal modello appena allenato. La funzione ci restituirà il dataframe \"prediction\", che conterrà il valore \"close\" per gli 11 giorni previsti a partire dalla sliding window, che inzia dagli ultimi 12 giorni di dati storici. \n",
    "Per salvare il modello \"lr\" nell'apposita cartella contenente il model dello specifico settore. chiamiamo sulla sua variabile il metodo \"write\", poi \"overwrite\" e poi \"save\" con il path su cui salvarlo.\n",
    "Popoliamo adesso quei due array vuoti specificati all'inizio del codice, che conterranno oggetti di tipo dictionary (i quali elasticsearch potrà accettare), che nel primo caso saranno costituiti dalle n_uple del settore specifico corrispondenti alla colonna \"close\", nell'altro saranno corrispondenti alla colonna \"prediction\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee90338-84cd-445a-bbfb-064a9a1cd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(indexes)):\n",
    "    print(\"sending index \"+names[i]+\" to elasticsearch...\")\n",
    "    save_and_send_data(\"prediction\",prediction_data,names[i],indexes[i])\n",
    "    save_and_send_data(\"historical\",historical_data,names[i],indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f00db-e5ee-40b2-bac6-65ec9e2e91b6",
   "metadata": {},
   "source": [
    "Per ognuno degli oggetti di tipo Elasticsearch nell'array indexes, inviamo la specifica di quale dei due indici, due per settore, vogliamo popolare, dell'oggetto di tipo dictionary, del nome del settore a cui è associato e del nome dell'oggetto elasticsearch a cui vogliamo inviare i dati."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
